
# ğŸ§  trainâ€‘t5â€‘textâ€‘toâ€‘sqlâ€‘lora

Má»™t implementation sá»­ dá»¥ng **LoRA** Ä‘á»ƒ fineâ€‘tune mÃ´ hÃ¬nh **FLANâ€‘T5â€‘Large** chuyÃªn cho nhiá»‡m vá»¥ chuyá»ƒn Ä‘á»•i cÃ¢u há»i tá»± nhiÃªn thÃ nh cÃ¢u SQL.

---

## ğŸ“ MÃ´ táº£

- DÃ¹ng `gretelai/synthetic_text_to_sql` lÃ m dataset huáº¥n luyá»‡n.  
- Chá»‰ dÃ¹ng **LoRA adapter** (~1.2% tham sá»‘) Ä‘á»ƒ fineâ€‘tune, tiáº¿t kiá»‡m bá»™ nhá»›.  
- Káº¿t quáº£ log vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng lÃªn **Wandb** vÃ  **Hugging Face Hub**.
- CÃ¡c metric Ä‘o Ä‘Æ°á»£c: {'bleu': 0.507682326324316, 'precisions': [0.7008027976819882, 0.5625887219149248, 0.4793407834700884, 0.4085876227509392]}
---

## ğŸ’» CÃ i Ä‘áº·t

```bash
git clone https://github.com/ngocnamk3er/train-t5-text-to-sql-lora.git
cd train-t5-text-to-sql-lora

pip install -q wandb transformers datasets accelerate peft bitsandbytes torch tensorboard huggingface_hub
pip install -U q transformers
```